{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7537d75b-e28e-40e7-9fc1-2b1a9aab1f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-0.9986062906572291 -1.6071732966272037 … -0.040289984671930436 -0.0541382873262382; -0.5926133644781323 0.9303213000912606 … 0.05083592049633394 -0.5397272071188509; … ; -0.0843610219990747 0.9426651021574727 … 0.529413990194043 -2.111582695432868; -0.6150401660464266 -0.9107616929153597 … 0.980329098883167 0.45714379354067725], [-7.134137084909282e-5 0.03323331656099597 … -0.20788117675945203 0.10651641861266287; -0.5641905267676229 -0.4136366046043286 … 1.2608057667501846 -1.923973329027189; … ; -1.5819186459448853 1.3989906673382813 … 2.0710203123520983 -2.321921426108096; -0.7334469141003682 0.3825677480073386 … 0.9087935352286151 -0.44773324929542957]], [[-2.820737269536844, -0.3303650089642246, -2.099063486302453, 2.134106389155389, -1.5991600134466966, -0.7286349167101922, -1.5840008523347413, 1.0610665290849683, -1.2582119894435178, -0.46447079373732114  …  -3.001110527504845, -1.3221895599392397, 1.0550639577100467, 0.23248384549234383, -1.6926080344442258, -0.15201300124582406, -0.9061353059898263, -0.12688412522127757, -1.4136945775074377, -1.446236413816416], [-1.198911490560231, -1.302609215467115, 0.27576935838607114, -2.5125007995500956, -1.0537595232480108, 0.9484005883792088, 0.7678068104687502, -0.5919597343728316, 0.17959229606900823, -1.0886759769623429]], [[0.1563931497143461 0.1563931497143461 … 0.1563931497143461 0.1563931497143461; 0.1563931497143461 0.1563931497143461 … 0.1563931497143461 0.1563931497143461; … ; 0.1563931497143461 0.1563931497143461 … 0.1563931497143461 0.1563931497143461; 0.1563931497143461 0.1563931497143461 … 0.1563931497143461 0.1563931497143461], [0.05231194849776563 0.05231194849776563 … 0.05231194849776563 0.05231194849776563; 0.05231194849776563 0.05231194849776563 … 0.05231194849776563 0.05231194849776563; … ; 0.05231194849776563 0.05231194849776563 … 0.05231194849776563 0.05231194849776563; 0.05231194849776563 0.05231194849776563 … 0.05231194849776563 0.05231194849776563]], [[0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323  …  0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323, 0.6750249560032323], [0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091, 0.2257891141501091]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n",
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    a = [x,zeros(30), zeros(10)]\n",
    "    z = [x, zeros(30), zeros(10)]\n",
    "    z[2] = W[1] * x + b[1]\n",
    "    a[2] = f.(z[2])\n",
    "    z[3] = W[2] * a[2] + b[2]\n",
    "    a[3] = f.(z[3])\n",
    "    return a, z\n",
    "end\n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    find_max = findmax(x)[2]\n",
    "    return find_max\n",
    "end\n",
    "\n",
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "\n",
    "    ∇W = δ[3] .* transpose(a[3])\n",
    "    ∇b = δ[3]\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "    \n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "            zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "\n",
    "    \n",
    "    for i in batch\n",
    "        x = trainx[i, :]\n",
    "        y = trainy[i]\n",
    "        ∇W, ∇b = backprop(W, b, x, y)\n",
    "        for j in 1:2\n",
    "            sumW[j] .+= ∇W[j]\n",
    "            sumb[j] .+= ∇b[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "\n",
    "    for k in 1:length(W)\n",
    "    \n",
    "        W[k] = W[k] .- α * ((1/m) * sumW[k] + λ*W[k])\n",
    "        b[k] = b[k] .- α * (1/m) * sumb[k]\n",
    "    end\n",
    "\n",
    "    \n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, sumW, sumb\n",
    "end\n",
    "\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b)\n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01)\n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    for e in epochs\n",
    "        remaining = 1:N\n",
    "        while length(remaining) > 0\n",
    "            batch = sample(remaining, m, replace=false)\n",
    "            remaining = setdiff(remaining, batch)\n",
    "            W, b, ∇W, ∇b = GD(W,b,batch)\n",
    "        end\n",
    "        acc = accuracy(W,b)\n",
    "    end\n",
    "\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 25       # batch size\n",
    "epochs = 10  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed389870-d2be-40cc-8828-2abb665caa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1 exceeds the tolerance: 0.16637921253544513\n",
      "Component 2 exceeds the tolerance: 0.16231928346695562\n",
      "Component 3 exceeds the tolerance: 0.14638324427647725\n",
      "Component 4 exceeds the tolerance: 0.14542721620199367\n",
      "Component 5 exceeds the tolerance: 0.16146677407942783\n",
      "Component 6 exceeds the tolerance: 0.153695782549163\n",
      "Component 7 exceeds the tolerance: 0.1654202234872731\n",
      "Component 8 exceeds the tolerance: 0.16795305822921486\n",
      "Component 9 exceeds the tolerance: 0.14934543903262815\n",
      "Component 10 exceeds the tolerance: 0.14695871070654115\n",
      "Component 11 exceeds the tolerance: 0.15543587250573304\n",
      "Component 12 exceeds the tolerance: 0.15678661950442924\n",
      "Component 13 exceeds the tolerance: 0.15054264513577104\n",
      "Component 14 exceeds the tolerance: 0.1514746276624795\n",
      "Component 15 exceeds the tolerance: 0.1516154129658947\n",
      "Component 16 exceeds the tolerance: 0.15370173860259123\n",
      "Component 17 exceeds the tolerance: 0.1527695527185518\n",
      "Component 18 exceeds the tolerance: 0.150106398043987\n",
      "Component 19 exceeds the tolerance: 0.1483974851394681\n",
      "Component 20 exceeds the tolerance: 0.16222519083993242\n",
      "Component 21 exceeds the tolerance: 0.15831635825787665\n",
      "Component 22 exceeds the tolerance: 0.16860742836951098\n",
      "Component 23 exceeds the tolerance: 0.1553139807468245\n",
      "Component 24 exceeds the tolerance: 0.1570871856897032\n",
      "Component 25 exceeds the tolerance: 0.16238515383389435\n",
      "Component 26 exceeds the tolerance: 0.1611443152908681\n",
      "Component 27 exceeds the tolerance: 0.1684842645969969\n",
      "Component 28 exceeds the tolerance: 0.17239198418491136\n",
      "Component 29 exceeds the tolerance: 0.1572367600514884\n",
      "Component 30 exceeds the tolerance: 0.16254355143115512\n",
      "Component 31 exceeds the tolerance: 0.17246488281426658\n",
      "Component 32 exceeds the tolerance: 0.14708993658115227\n",
      "Component 33 exceeds the tolerance: 0.13822698205086575\n",
      "Component 34 exceeds the tolerance: 0.16075270541745007\n",
      "Component 35 exceeds the tolerance: 0.1551491097499696\n",
      "Component 36 exceeds the tolerance: 0.1755571826349172\n",
      "Component 37 exceeds the tolerance: 0.14855108366003728\n",
      "Component 38 exceeds the tolerance: 0.16285357195596256\n",
      "Component 39 exceeds the tolerance: 0.16007095074695493\n",
      "Component 40 exceeds the tolerance: 0.16379963893183352\n",
      "Component 41 exceeds the tolerance: 0.1786653931416698\n",
      "Component 42 exceeds the tolerance: 0.14209439830477671\n",
      "Component 43 exceeds the tolerance: 0.1552496309413403\n",
      "Component 44 exceeds the tolerance: 0.16379987063981966\n",
      "Component 45 exceeds the tolerance: 0.15959256179183862\n",
      "Component 46 exceeds the tolerance: 0.155395349827187\n",
      "Component 47 exceeds the tolerance: 0.15766893701237344\n",
      "Component 48 exceeds the tolerance: 0.1515303433079101\n",
      "Component 49 exceeds the tolerance: 0.1458742728514711\n",
      "Component 50 exceeds the tolerance: 0.16127945973761265\n",
      "Component 51 exceeds the tolerance: 0.15532292562721645\n",
      "Component 52 exceeds the tolerance: 0.15047876080683303\n",
      "Component 53 exceeds the tolerance: 0.1542440722827781\n",
      "Component 54 exceeds the tolerance: 0.15663189676314354\n",
      "Component 55 exceeds the tolerance: 0.15375808577840538\n",
      "Component 56 exceeds the tolerance: 0.16131855408315127\n",
      "Component 57 exceeds the tolerance: 0.15989797792866406\n",
      "Component 58 exceeds the tolerance: 0.1548100150243375\n",
      "Component 59 exceeds the tolerance: 0.14696649868125072\n",
      "Component 60 exceeds the tolerance: 0.16550076656170526\n",
      "Component 61 exceeds the tolerance: 0.16217545895909674\n",
      "Component 62 exceeds the tolerance: 0.15852441170790038\n",
      "Component 63 exceeds the tolerance: 0.1414637532574819\n",
      "Component 64 exceeds the tolerance: 0.16243043864090972\n",
      "Component 65 exceeds the tolerance: 0.17253934314550073\n",
      "Component 66 exceeds the tolerance: 0.15824886586398074\n",
      "Component 67 exceeds the tolerance: 0.1637854848494829\n",
      "Component 68 exceeds the tolerance: 0.16080044799178844\n",
      "Component 69 exceeds the tolerance: 0.14223779222373772\n",
      "Component 70 exceeds the tolerance: 0.14725307058523535\n",
      "Component 71 exceeds the tolerance: 0.14841457575283706\n",
      "Component 72 exceeds the tolerance: 0.1639471793650376\n",
      "Component 73 exceeds the tolerance: 0.1694973780301428\n",
      "Component 74 exceeds the tolerance: 0.16057394714704182\n",
      "Component 75 exceeds the tolerance: 0.16384916155783352\n",
      "Component 76 exceeds the tolerance: 0.16288739137433944\n",
      "Component 77 exceeds the tolerance: 0.16763875455279625\n",
      "Component 78 exceeds the tolerance: 0.15675306497638425\n",
      "Component 79 exceeds the tolerance: 0.1472210664614398\n",
      "Component 80 exceeds the tolerance: 0.15996290746596636\n",
      "Component 81 exceeds the tolerance: 0.140073302836014\n",
      "Component 82 exceeds the tolerance: 0.16894124312312792\n",
      "Component 83 exceeds the tolerance: 0.1546183649507487\n",
      "Component 84 exceeds the tolerance: 0.15756108877536584\n",
      "Component 85 exceeds the tolerance: 0.16733570921286295\n",
      "Component 86 exceeds the tolerance: 0.16985273942378748\n",
      "Component 87 exceeds the tolerance: 0.14983340333270584\n",
      "Component 88 exceeds the tolerance: 0.16253836695608365\n",
      "Component 89 exceeds the tolerance: 0.16412886109247787\n",
      "Component 90 exceeds the tolerance: 0.15268469231540288\n",
      "Component 91 exceeds the tolerance: 0.15119415050986512\n",
      "Component 92 exceeds the tolerance: 0.15727755564949925\n",
      "Component 93 exceeds the tolerance: 0.14434682417726102\n",
      "Component 94 exceeds the tolerance: 0.15421323401750336\n",
      "Component 95 exceeds the tolerance: 0.16331326518166983\n",
      "Component 96 exceeds the tolerance: 0.15588137065077384\n",
      "Component 97 exceeds the tolerance: 0.15141481566712295\n",
      "Component 98 exceeds the tolerance: 0.16304275303565313\n",
      "Component 99 exceeds the tolerance: 0.17055735296463476\n",
      "Component 100 exceeds the tolerance: 0.1502408169555441\n",
      "Component 101 exceeds the tolerance: 0.16639577009965373\n",
      "Component 102 exceeds the tolerance: 0.15698235903567942\n",
      "Component 103 exceeds the tolerance: 0.17048287231187836\n",
      "Component 104 exceeds the tolerance: 0.16164706427997438\n",
      "Component 105 exceeds the tolerance: 0.15720060436859634\n",
      "Component 106 exceeds the tolerance: 0.15446777543655693\n",
      "Component 107 exceeds the tolerance: 0.16592636929698196\n",
      "Component 108 exceeds the tolerance: 0.15782207971977763\n",
      "Component 109 exceeds the tolerance: 0.161468552710004\n",
      "Component 110 exceeds the tolerance: 0.16117956879764855\n",
      "Component 111 exceeds the tolerance: 0.16224726512899573\n",
      "Component 112 exceeds the tolerance: 0.1442793506127476\n",
      "Component 113 exceeds the tolerance: 0.16762976121338974\n",
      "Component 114 exceeds the tolerance: 0.15566791899442373\n",
      "Component 115 exceeds the tolerance: 0.17181068363055269\n",
      "Component 116 exceeds the tolerance: 0.16048933912867264\n",
      "Component 117 exceeds the tolerance: 0.1647924695673158\n",
      "Component 118 exceeds the tolerance: 0.14912776227600838\n",
      "Component 119 exceeds the tolerance: 0.15112535185420378\n",
      "Component 120 exceeds the tolerance: 0.1688936845793581\n",
      "Component 121 exceeds the tolerance: 0.15468097591604996\n",
      "Component 122 exceeds the tolerance: 0.16503316584800026\n",
      "Component 123 exceeds the tolerance: 0.15422670512617637\n",
      "Component 124 exceeds the tolerance: 0.16551518404224802\n",
      "Component 125 exceeds the tolerance: 0.17163099255964093\n",
      "Component 126 exceeds the tolerance: 0.169689525838918\n",
      "Component 127 exceeds the tolerance: 0.16630974533012371\n",
      "Component 128 exceeds the tolerance: 0.16157581780458835\n",
      "Component 129 exceeds the tolerance: 0.17041771199029548\n",
      "Component 130 exceeds the tolerance: 0.16492728737756474\n",
      "Component 131 exceeds the tolerance: 0.15843099011743683\n",
      "Component 132 exceeds the tolerance: 0.15251352662045187\n",
      "Component 133 exceeds the tolerance: 0.14535556223034868\n",
      "Component 134 exceeds the tolerance: 0.14997988235717125\n",
      "Component 135 exceeds the tolerance: 0.15290245682405051\n",
      "Component 136 exceeds the tolerance: 0.1624043809071612\n",
      "Component 137 exceeds the tolerance: 0.1562345187748509\n",
      "Component 138 exceeds the tolerance: 0.16487559191187878\n",
      "Component 139 exceeds the tolerance: 0.15266120788322618\n",
      "Component 140 exceeds the tolerance: 0.16895277004476733\n",
      "Component 141 exceeds the tolerance: 0.17306362570064843\n",
      "Component 142 exceeds the tolerance: 0.16295974857013001\n",
      "Component 143 exceeds the tolerance: 0.15367408236085006\n",
      "Component 144 exceeds the tolerance: 0.15686832886370788\n",
      "Component 145 exceeds the tolerance: 0.1654386302389515\n",
      "Component 146 exceeds the tolerance: 0.16603355289231453\n",
      "Component 147 exceeds the tolerance: 0.17369195629709006\n",
      "Component 148 exceeds the tolerance: 0.15003551927446673\n",
      "Component 149 exceeds the tolerance: 0.15693231771348232\n",
      "Component 150 exceeds the tolerance: 0.1679888791754241\n",
      "Component 151 exceeds the tolerance: 0.1600559284524355\n",
      "Component 152 exceeds the tolerance: 0.17075703799758807\n",
      "Component 153 exceeds the tolerance: 0.16197313092032922\n",
      "Component 154 exceeds the tolerance: 0.15223467270271734\n",
      "Component 155 exceeds the tolerance: 0.16347832091963604\n",
      "Component 156 exceeds the tolerance: 0.14507287671091157\n",
      "Component 157 exceeds the tolerance: 0.1739604846077885\n",
      "Component 158 exceeds the tolerance: 0.16348678021512236\n",
      "Component 159 exceeds the tolerance: 0.1614035787637807\n",
      "Component 160 exceeds the tolerance: 0.14365408124757445\n",
      "Component 161 exceeds the tolerance: 0.16254162820513224\n",
      "Component 162 exceeds the tolerance: 0.165891249159073\n",
      "Component 163 exceeds the tolerance: 0.1496163156870683\n",
      "Component 164 exceeds the tolerance: 0.16128990848170482\n",
      "Component 165 exceeds the tolerance: 0.16440546933989603\n",
      "Component 166 exceeds the tolerance: 0.14288721869189552\n",
      "Component 167 exceeds the tolerance: 0.15474661812771623\n",
      "Component 168 exceeds the tolerance: 0.15993118251441168\n",
      "Component 169 exceeds the tolerance: 0.1466599566184062\n",
      "Component 170 exceeds the tolerance: 0.1768429151153999\n",
      "Component 171 exceeds the tolerance: 0.16156467358126642\n",
      "Component 172 exceeds the tolerance: 0.15361986553426626\n",
      "Component 173 exceeds the tolerance: 0.15220048714166873\n",
      "Component 174 exceeds the tolerance: 0.14932785892636852\n",
      "Component 175 exceeds the tolerance: 0.16247685782940274\n",
      "Component 176 exceeds the tolerance: 0.17261216411253494\n",
      "Component 177 exceeds the tolerance: 0.15944718801659868\n",
      "Component 178 exceeds the tolerance: 0.15807802190278047\n",
      "Component 179 exceeds the tolerance: 0.15630675553364776\n",
      "Component 180 exceeds the tolerance: 0.16579203110307966\n",
      "Component 181 exceeds the tolerance: 0.14972913381524117\n",
      "Component 182 exceeds the tolerance: 0.16666027112518017\n",
      "Component 183 exceeds the tolerance: 0.15630933487483284\n",
      "Component 184 exceeds the tolerance: 0.16142287497285573\n",
      "Component 185 exceeds the tolerance: 0.17437397493421153\n",
      "Component 186 exceeds the tolerance: 0.15715406737211773\n",
      "Component 187 exceeds the tolerance: 0.1616521131124376\n",
      "Component 188 exceeds the tolerance: 0.1619599403339283\n",
      "Component 189 exceeds the tolerance: 0.1591711625563324\n",
      "Component 190 exceeds the tolerance: 0.16500780082236483\n",
      "Component 191 exceeds the tolerance: 0.15628787769213554\n",
      "Component 192 exceeds the tolerance: 0.1604812724791021\n",
      "Component 193 exceeds the tolerance: 0.16499395532872435\n",
      "Component 194 exceeds the tolerance: 0.16864272223511884\n",
      "Component 195 exceeds the tolerance: 0.17348846325245298\n",
      "Component 196 exceeds the tolerance: 0.1607920116468325\n",
      "Component 197 exceeds the tolerance: 0.15426373683720734\n",
      "Component 198 exceeds the tolerance: 0.14950051484259924\n",
      "Component 199 exceeds the tolerance: 0.17329918937588112\n",
      "Component 200 exceeds the tolerance: 0.16713499097971757\n",
      "200 components exceeded the tolerance of 0.001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# unroll the weights and biases into a single vector.\n",
    "# note this function will also work for unrolling the gradient.\n",
    "# note that this is hard-coded for a 3-layer NN.\n",
    "function unroll(W, b)\n",
    "    vcat(vec(W[1]), vec(W[2]), vec(b[1]), vec(b[2]))\n",
    "end\n",
    "\n",
    "# given a single vector θ, reshape the parameters into the data\n",
    "# structures that are used for backpropagation, that is, W and b, or\n",
    "# ∇w and ∇b.  note that this is hard-coded for a 3-layer NN.\n",
    "function reshape_params(θ)\n",
    "    n1 = sizes[1]  # number of nodes in layer 1\n",
    "    n2 = sizes[2]  # number of nodes in layer 2\n",
    "    n3 = sizes[3]\n",
    "    W1 = reshape(θ[1:(n2*n1)], n2, n1)\n",
    "    W2 = reshape(θ[(n2*n1 + 1):(n2*n1 + n2*n3)], n3, n2)\n",
    "    b1 = θ[(n2*n1 + n2*n3 + 1):(n2*n1 + n2*n3 + n2)]\n",
    "    b2 = θ[(n2*n1 + n2*n3 + n2 + 1):length(θ)]\n",
    "    W = [ W1, W2 ]\n",
    "    b = [ b1, b2 ]\n",
    "    return W, b\n",
    "end\n",
    "\n",
    "# evaluate the cost function for a batch of training examples\n",
    "# θ is the unrolled vector of weights and biases.\n",
    "# batch is the set of indices of the batch of training examples.\n",
    "function J(θ, batch, λ)\n",
    "\n",
    "    m = length(batch)\n",
    "    sumJ = 0.0  # to accumulate the sum for the batch.\n",
    "    # we need to pass W, b to feedforward, so we re-create W, b from θ\n",
    "    W, b = reshape_params(θ)\n",
    "    for i in batch\n",
    "        x = trainx[i, :]  \n",
    "        y = trainy[i]\n",
    "        a, _ = feedforward(W, b, x)  # Calculate activations\n",
    "        sumJ += (1/2) * sum((a[end] .- digit2vector(y)).^2)\n",
    "    end\n",
    "         \n",
    "\n",
    "        # accumulate the cost function \n",
    "    ans = 0.0\n",
    "    for l in 1:length(W)\n",
    "        ans += sum(W[l] .^ 2)\n",
    "    end\n",
    "\n",
    "    func = (1/m) * sumJ + (λ/2) * ans\n",
    "    return func\n",
    "\n",
    "    # return the cost function. note that the regularization term only\n",
    "    # applies to the weights, not the biases\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# create the ith basis vector\n",
    "function e(i)\n",
    "    e = zeros(sizes[2]*sizes[1] + sizes[3]*sizes[2] + sizes[2] + sizes[3])\n",
    "    e[i] = 1\n",
    "    return e\n",
    "end\n",
    "\n",
    "θplus(v, i; ϵ=1e-4) = v .+ ϵ*e(i)\n",
    "θminus(v, i; ϵ=1e-4) = v .- ϵ*e(i)\n",
    "\n",
    "# compute the difference between the ith element of the gradient as\n",
    "# computed from backpropagation (this is ∇θ[i]) and the approximation of\n",
    "# the ith element of the gradient as obtained from finite differencing.\n",
    "# the idea is to see if the backpropagation code is correctly computing\n",
    "# the gradient of the cost function.\n",
    "function compare1(i, θ, ∇θ, batch, λ; ϵ=1e-4)\n",
    "    # i is the ith element of the unrolled gradient θ,\n",
    "    ∇θ[i] - ( J(θplus(θ, i, ϵ=ϵ), batch, λ) - J(θminus(θ, i, ϵ=ϵ), batch, λ) )/(2*ϵ)\n",
    "end\n",
    "\n",
    "# compare each element of the gradient as computed from\n",
    "# backpropagation to its estimate as obtained from finite\n",
    "# differencing.\n",
    "function compare(W, b, ∇W, ∇b, λ)\n",
    "    θ = unroll(W, b)\n",
    "    ∇θ = unroll(∇W, ∇b)\n",
    "    m = length(trainy)\n",
    "\n",
    "    batch = rand(1:m, 5000)\n",
    "    tolerance = 0.001\n",
    "    number_components_exceeding_tolerance = 0\n",
    "\n",
    "    for i in 1:200\n",
    "        finite_diff = compare1(i, θ, ∇θ, batch, λ)  \n",
    "        if abs(finite_diff) > tolerance\n",
    "            number_components_exceeding_tolerance += 1\n",
    "            println(\"Component $i exceeds the tolerance: $finite_diff\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"$number_components_exceeding_tolerance components exceeded the tolerance of $tolerance\")\n",
    "end\n",
    "\n",
    "compare(W, b, ∇W, ∇b, λ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
