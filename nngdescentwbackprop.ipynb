{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72411f89-55d8-4c5c-beb0-a37183680262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fprime (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the activation function\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea2883c-c79e-4da9-8e1c-bc011f71cc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "digit2vector (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ce41a9-f6ef-4f02-885e-555ac3006a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    a = [x,zeros(30), zeros(10)]\n",
    "    z = [x, zeros(30), zeros(10)]\n",
    "    z[2] = W[1] * x + b[1]\n",
    "    a[2] = f.(z[2])\n",
    "    z[3] = W[2] * a[2] + b[2]\n",
    "    a[3] = f.(z[3])\n",
    "    return a, z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9321710-7b99-4e20-902e-b4f27dd469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feed forward is calculated by hard coding the appropriate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35020f2-9ea3-4d17-ad7d-5e2b13955b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classify (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    find_max = findmax(x)[2]\n",
    "    return find_max\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77eb0fab-6071-469e-b6d6-02cb6522f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_error (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3703208-3fc1-4709-b57e-0af4b928eadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_gradients (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "\n",
    "    ∇W = δ[3] .* transpose(a[3])\n",
    "    ∇b = δ[3]\n",
    "    return ∇W, ∇b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89076bd0-27cf-41ab-a587-9508f09317d9",
   "metadata": {},
   "source": [
    "This computes the gradients following the BG3 and BG4 methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2c431c-79fb-4c4d-9acc-c1c4e1686fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8551c350-f1d2-48a7-9197-24fabcd49576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "    \n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "            zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "\n",
    "    \n",
    "    for i in batch\n",
    "        x = trainx[i, :]\n",
    "        y = trainy[i]\n",
    "        ∇W, ∇b = backprop(W, b, x, y)\n",
    "        for j in 1:2\n",
    "            sumW[j] .+= ∇W[j]\n",
    "            sumb[j] .+= ∇b[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "\n",
    "    for k in 1:length(W)\n",
    "    \n",
    "        W[k] = W[k] .- α * ((1/m) * sumW[k] + λ*W[k])\n",
    "        b[k] = b[k] .- α * (1/m) * sumb[k]\n",
    "    end\n",
    "\n",
    "    \n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, sumW, sumb\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5143760-24f4-473e-b0f1-a24c0fb51684",
   "metadata": {},
   "source": [
    "This is the gradient descent function. x and y are initialized through the MTRAIN data set that we imported in the beginning, and their values are summed for each element in the batch. I then iterate through the length of the weights in the network (the amount of weights), and once again sum the average gradient to each weight. These values are then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019aed3e-b928-4022-a258-f0f834ad77a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b)\n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "276d6f21-f37d-41fd-a057-82d86402d6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 2-element Vector{Matrix{Float64}} at index [3]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 2-element Vector{Matrix{Float64}} at index [3]",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ .\\essentials.jl:13 [inlined]",
      " [2] maybeview",
      "   @ .\\views.jl:149 [inlined]",
      " [3] dotview",
      "   @ .\\broadcast.jl:1214 [inlined]",
      " [4] GD(W::Vector{Matrix{Float64}}, b::Vector{Vector{Float64}}, batch::Vector{Int64}; α::Float64, λ::Float64)",
      "   @ Main .\\In[20]:25",
      " [5] GD",
      "   @ .\\In[20]:7 [inlined]",
      " [6] BGD(N::Int64, m::Int64, epochs::Int64; α::Float64, λ::Float64)",
      "   @ Main .\\In[22]:18",
      " [7] top-level scope",
      "   @ In[22]:33"
     ]
    }
   ],
   "source": [
    "function BGD(N, m, epochs; α=0.01, λ=0.01)\n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    for e in epochs\n",
    "        remaining = 1:N\n",
    "        while length(remaining) > 0\n",
    "            batch = sample(remaining, m, replace=false)\n",
    "            remaining = setdiff(remaining, batch)\n",
    "            W, b, ∇W, ∇b = GD(W,b,batch)\n",
    "        end\n",
    "        acc = accuracy(W,b)\n",
    "        println(acc)\n",
    "    end\n",
    "\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 25       # batch size\n",
    "epochs = 10  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
